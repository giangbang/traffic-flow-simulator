{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import optparse\n",
    "from environment import environment\n",
    "from sumolib import checkBinary  # Checks for the binary in environ vars\n",
    "import traci\n",
    "from agent import agents\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added SUMO_HOME to tools directory\n",
      "C:\\Program Files (x86)\\Eclipse\\Sumo\\\n"
     ]
    }
   ],
   "source": [
    "if 'SUMO_HOME' in os.environ:\n",
    "    tools = os.path.join(os.environ['SUMO_HOME'], 'tools')\n",
    "    sys.path.append(tools)\n",
    "    print('added SUMO_HOME to tools directory')\n",
    "else:\n",
    "    sys.exit(\"please declare environment variable 'SUMO_HOME'\")\n",
    "\n",
    "print(os.environ['SUMO_HOME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Eclipse\\Sumo\\bin\\sumo\n",
      "<class 'str'>\n",
      "C:\\Program Files (x86)\\Eclipse\\Sumo\\bin\\sumo-gui\n",
      "C:\\Program Files (x86)\\Eclipse\\Sumo\\\n"
     ]
    }
   ],
   "source": [
    "sumoBinaryNoGui = checkBinary('sumo')\n",
    "sumoBinary = checkBinary('sumo-gui')\n",
    "\n",
    "nogui = [sumoBinaryNoGui, \"-c\", \"test.sumocfg\", '--no-warnings']\n",
    "gui = [sumoBinary, \"-c\", \"test.sumocfg\", '--quit-on-end']\n",
    "\n",
    "print(sumoBinaryNoGui)\n",
    "print(type(sumoBinary))\n",
    "print(sumoBinary)\n",
    "print(os.environ['SUMO_HOME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_run():\n",
    "    traci.start(gui)\n",
    "    step = 0\n",
    "    env = environment()\n",
    "    traci.trafficlight.setPhase('0', 0)\n",
    "    action = [0]\n",
    "    while traci.simulation.getMinExpectedNumber() > 0:\n",
    "        traci.simulationStep()\n",
    "        step += 1\n",
    "    print('total waiting time of defaul cyclic traffic light', step)\n",
    "    traci.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = None\n",
    "env = None\n",
    "# contains TraCI control loop\n",
    "def run(episodes):\n",
    "                             \n",
    "    traci.start(nogui)\n",
    "    env = environment()\n",
    "    agent = agents(env.state_size(), env.action_size())\n",
    "#     agent.load_policy('model.pt')\n",
    "    traci.close()\n",
    "    epsilon = 0.1\n",
    "    ep = 0.\n",
    "    while ep < episodes:\n",
    "        ep+=1\n",
    "        print(ep)\n",
    "        epsilon = ep / (episodes + .1)\n",
    "\n",
    "#         if (ep == episodes - 1):\n",
    "#             traci.start(gui)\n",
    "#         else: \n",
    "        traci.start(gui)\n",
    "        step = 0\n",
    "        prev = env.getState()\n",
    "        reward = env.reward()\n",
    "        action = agent.select_actions(1 , prev)\n",
    "        print(action)\n",
    "        print(epsilon)\n",
    "        while traci.simulation.getMinExpectedNumber() > 0 and step < 5e3:\n",
    "            traci.simulationStep()\n",
    "            env.do_action(action)\n",
    "            env.cumulateWaitingTime()\n",
    "            if (step % 20 == 0):\n",
    "                state = env.getState()\n",
    "                reward = env.reward()\n",
    "                print('reward:', reward)\n",
    "                agent.add_memmory(prev, action, state, reward)\n",
    "                action = agent.select_actions(epsilon , state)\n",
    "                \n",
    "            agent.train()\n",
    "            step += 1\n",
    "        print(len(agent.mem))      \n",
    "        if (step >= 5e3-1):\n",
    "            print('fail to complete the episode')\n",
    "        print(step)\n",
    "        traci.close()\n",
    "        \n",
    "    agent.save_policy('model.pt')\n",
    "    sys.stdout.flush()\n",
    "    agent.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': ['GGGGGgrrrrrrrrgGGGGgrrrrrrrGr', 'GGGGGgrrrrrrrrgGGGGgrrrrrrrrr', 'yyyyygrrrrrrrryyyyygrrrrrrrrr', 'rrrrrGrrrrrrrrrrrrrGrrrrrrrrr', 'rrrrryrrrrrrrrrrrrryrrrrrrrrr', 'rrrrrrGGGGGGGgrrrrrrgGGGGggrG', 'rrrrrrGGGGGGGgrrrrrrgGGGGggrr', 'rrrrrryyyyyyyyrrrrrryyyyyyyrr']}\n",
      "{'0': {'31_2', '41_2', '11_1', '21_1', '41_1', '21_3', ':0_w2_0', '11_3', '21_2', '11_2', '11_0', '31_0', '31_3', '41_0', '21_4', '21_0', '41_3', '31_1', ':0_w1_0'}}\n",
      "device available:  cpu\n",
      "1.0\n",
      "[[2]]\n",
      "0.47619047619047616\n",
      "reward: [[0.]]\n",
      "reward: [[0.]]\n",
      "reward: [[-0.001]]\n",
      "reward: [[-0.049]]\n",
      "reward: [[-0.155]]\n",
      "reward: [[-0.208]]\n",
      "reward: [[-0.36]]\n",
      "reward: [[-0.624]]\n",
      "reward: [[-0.878]]\n",
      "reward: [[-1.119]]\n",
      "reward: [[-1.325]]\n",
      "reward: [[-1.609]]\n",
      "reward: [[-0.238]]\n",
      "reward: [[-1.182]]\n",
      "reward: [[-0.728]]\n",
      "reward: [[-1.396]]\n",
      "reward: [[-1.216]]\n",
      "reward: [[-2.126]]\n",
      "reward: [[-2.607]]\n",
      "reward: [[-0.592]]\n",
      "reward: [[-1.593]]\n",
      "reward: [[-1.938]]\n",
      "reward: [[-2.862]]\n",
      "reward: [[-0.177]]\n",
      "reward: [[-1.005]]\n",
      "reward: [[-1.831]]\n",
      "reward: [[-1.618]]\n",
      "reward: [[-2.32]]\n",
      "reward: [[-2.507]]\n",
      "reward: [[-2.675]]\n",
      "reward: [[-3.078]]\n",
      "reward: [[-3.657]]\n",
      "reward: [[-0.196]]\n",
      "reward: [[-1.563]]\n",
      "reward: [[-2.698]]\n",
      "reward: [[-3.887]]\n",
      "reward: [[-4.194]]\n",
      "reward: [[-4.479]]\n",
      "reward: [[-0.393]]\n",
      "reward: [[-0.141]]\n",
      "reward: [[-2.764]]\n",
      "reward: [[-2.987]]\n",
      "reward: [[-1.592]]\n",
      "reward: [[-3.12]]\n",
      "reward: [[-3.232]]\n",
      "reward: [[-3.784]]\n",
      "reward: [[-0.062]]\n",
      "reward: [[-0.375]]\n",
      "reward: [[-2.734]]\n",
      "reward: [[-3.974]]\n",
      "reward: [[-0.639]]\n",
      "reward: [[-0.885]]\n",
      "reward: [[-2.357]]\n",
      "reward: [[-1.998]]\n",
      "reward: [[-3.681]]\n",
      "reward: [[-3.909]]\n",
      "reward: [[-3.111]]\n",
      "reward: [[-4.358]]\n",
      "reward: [[-0.576]]\n",
      "reward: [[-2.548]]\n",
      "reward: [[-2.875]]\n",
      "reward: [[-2.207]]\n",
      "reward: [[-0.227]]\n",
      "reward: [[-1.85]]\n",
      "reward: [[-4.053]]\n",
      "reward: [[-4.782]]\n",
      "reward: [[-0.845]]\n",
      "reward: [[-0.919]]\n",
      "reward: [[-0.918]]\n",
      "reward: [[-4.179]]\n",
      "reward: [[-4.001]]\n",
      "reward: [[-0.254]]\n",
      "reward: [[-1.133]]\n",
      "reward: [[-2.805]]\n",
      "reward: [[-3.152]]\n",
      "reward: [[-2.95]]\n",
      "reward: [[-3.032]]\n",
      "reward: [[-0.363]]\n",
      "reward: [[-1.448]]\n",
      "reward: [[-2.506]]\n",
      "reward: [[-3.917]]\n",
      "reward: [[-4.407]]\n",
      "reward: [[-4.863]]\n",
      "reward: [[-4.508]]\n",
      "reward: [[-5.102]]\n",
      "reward: [[-5.226]]\n",
      "reward: [[-5.283]]\n",
      "reward: [[-0.566]]\n",
      "reward: [[-2.986]]\n",
      "reward: [[-3.377]]\n",
      "reward: [[-4.731]]\n",
      "reward: [[-5.296]]\n",
      "reward: [[-5.32]]\n",
      "reward: [[-0.266]]\n",
      "reward: [[-1.262]]\n",
      "reward: [[-3.868]]\n",
      "reward: [[-4.224]]\n",
      "reward: [[-2.548]]\n",
      "reward: [[-3.876]]\n",
      "reward: [[-0.507]]\n",
      "reward: [[-1.983]]\n",
      "reward: [[-3.144]]\n",
      "reward: [[-3.339]]\n",
      "reward: [[-0.164]]\n",
      "reward: [[-1.294]]\n",
      "reward: [[-2.455]]\n",
      "reward: [[-2.45]]\n",
      "reward: [[-2.76]]\n",
      "reward: [[-0.482]]\n",
      "reward: [[-2.274]]\n",
      "reward: [[-2.439]]\n",
      "reward: [[-2.544]]\n",
      "reward: [[-0.464]]\n",
      "reward: [[-0.304]]\n",
      "reward: [[-1.81]]\n",
      "reward: [[-1.538]]\n",
      "reward: [[-2.161]]\n",
      "reward: [[-1.278]]\n",
      "reward: [[-1.901]]\n",
      "reward: [[-1.973]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mr.Son\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:432: UserWarning: Using a target size (torch.Size([32, 32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: [[-2.169]]\n",
      "reward: [[-2.36]]\n",
      "reward: [[-2.36]]\n",
      "reward: [[-2.36]]\n",
      "reward: [[-2.36]]\n",
      "reward: [[-2.36]]\n",
      "reward: [[-0.569]]\n",
      "reward: [[-0.158]]\n",
      "reward: [[-1.504]]\n",
      "reward: [[-1.662]]\n",
      "reward: [[-2.068]]\n",
      "reward: [[-2.074]]\n",
      "reward: [[-2.085]]\n",
      "reward: [[-0.106]]\n",
      "reward: [[-0.394]]\n",
      "reward: [[-0.22]]\n",
      "reward: [[-0.942]]\n",
      "reward: [[-1.375]]\n",
      "reward: [[-1.503]]\n",
      "reward: [[-1.941]]\n",
      "reward: [[-2.03]]\n",
      "reward: [[-1.926]]\n",
      "reward: [[-2.115]]\n",
      "reward: [[-1.987]]\n",
      "reward: [[-1.931]]\n",
      "reward: [[-2.019]]\n",
      "reward: [[-2.115]]\n",
      "reward: [[-2.083]]\n",
      "reward: [[-2.101]]\n",
      "reward: [[-2.119]]\n",
      "reward: [[-2.16]]\n",
      "reward: [[-2.16]]\n",
      "reward: [[-2.16]]\n",
      "reward: [[-2.16]]\n",
      "reward: [[-2.16]]\n",
      "reward: [[-2.16]]\n",
      "reward: [[-2.16]]\n",
      "reward: [[-2.16]]\n",
      "reward: [[-2.16]]\n",
      "reward: [[-2.16]]\n",
      "reward: [[-2.16]]\n",
      "reward: [[-2.16]]\n",
      "reward: [[-2.16]]\n",
      "reward: [[-2.16]]\n",
      "reward: [[-2.16]]\n",
      "reward: [[-2.16]]\n",
      "reward: [[-2.16]]\n",
      "reward: [[-2.16]]\n",
      "reward: [[-2.16]]\n",
      "reward: [[-2.16]]\n",
      "reward: [[-0.212]]\n",
      "reward: [[-0.798]]\n",
      "reward: [[-1.53]]\n",
      "reward: [[-0.307]]\n",
      "reward: [[-0.822]]\n",
      "reward: [[-0.509]]\n",
      "reward: [[-0.255]]\n",
      "reward: [[-0.661]]\n",
      "reward: [[-1.309]]\n",
      "reward: [[-1.415]]\n",
      "reward: [[-1.42]]\n",
      "reward: [[-1.42]]\n",
      "reward: [[-1.42]]\n",
      "reward: [[-0.071]]\n",
      "reward: [[-0.461]]\n",
      "reward: [[-0.417]]\n",
      "reward: [[-0.614]]\n",
      "reward: [[-1.201]]\n",
      "reward: [[-1.22]]\n",
      "reward: [[-1.22]]\n",
      "reward: [[-1.22]]\n",
      "reward: [[-1.22]]\n",
      "reward: [[-1.22]]\n",
      "reward: [[-1.22]]\n",
      "reward: [[-1.22]]\n",
      "reward: [[-1.22]]\n",
      "reward: [[-1.22]]\n",
      "reward: [[-1.22]]\n",
      "reward: [[-1.22]]\n",
      "reward: [[-0.061]]\n",
      "reward: [[-0.065]]\n",
      "reward: [[-0.967]]\n",
      "reward: [[-1.036]]\n",
      "reward: [[-1.046]]\n",
      "reward: [[-1.031]]\n",
      "reward: [[-1.038]]\n",
      "reward: [[-1.06]]\n",
      "reward: [[-1.06]]\n",
      "reward: [[-1.06]]\n",
      "reward: [[-1.06]]\n",
      "reward: [[-1.06]]\n",
      "reward: [[-1.06]]\n",
      "reward: [[-0.276]]\n",
      "reward: [[-0.08]]\n",
      "reward: [[-0.275]]\n",
      "reward: [[-0.691]]\n",
      "reward: [[-0.879]]\n",
      "reward: [[-0.88]]\n",
      "reward: [[-0.88]]\n",
      "reward: [[-0.88]]\n",
      "reward: [[-0.88]]\n",
      "reward: [[-0.88]]\n",
      "reward: [[-0.88]]\n",
      "reward: [[-0.88]]\n",
      "reward: [[-0.88]]\n",
      "reward: [[-0.88]]\n",
      "reward: [[-0.88]]\n",
      "reward: [[-0.88]]\n",
      "reward: [[-0.88]]\n",
      "reward: [[-0.88]]\n",
      "reward: [[-0.88]]\n",
      "reward: [[-0.88]]\n",
      "reward: [[-0.88]]\n",
      "reward: [[-0.88]]\n",
      "reward: [[-0.88]]\n",
      "reward: [[-0.88]]\n",
      "reward: [[-0.88]]\n",
      "reward: [[-0.044]]\n",
      "reward: [[-0.15]]\n",
      "reward: [[-0.085]]\n",
      "reward: [[-0.122]]\n",
      "reward: [[-0.463]]\n",
      "reward: [[-0.52]]\n",
      "reward: [[-0.52]]\n",
      "reward: [[-0.52]]\n",
      "reward: [[-0.52]]\n",
      "reward: [[-0.52]]\n",
      "reward: [[-0.52]]\n",
      "reward: [[-0.52]]\n",
      "reward: [[-0.52]]\n",
      "250\n",
      "fail to complete the episode\n",
      "5000\n",
      "2.0\n",
      "[[1]]\n",
      "0.9523809523809523\n",
      "reward: [[0.]]\n",
      "reward: [[0.]]\n",
      "reward: [[-0.001]]\n",
      "reward: [[-0.09]]\n",
      "reward: [[-0.208]]\n",
      "reward: [[-0.373]]\n",
      "reward: [[-0.534]]\n",
      "reward: [[-0.577]]\n",
      "reward: [[-0.861]]\n",
      "reward: [[-0.986]]\n",
      "reward: [[-1.249]]\n",
      "reward: [[-1.237]]\n",
      "reward: [[-1.112]]\n",
      "reward: [[-1.75]]\n",
      "reward: [[-1.786]]\n",
      "reward: [[-1.977]]\n",
      "reward: [[-1.585]]\n",
      "reward: [[-0.114]]\n",
      "reward: [[-0.29]]\n",
      "reward: [[-1.542]]\n",
      "reward: [[-2.219]]\n",
      "reward: [[-2.72]]\n",
      "reward: [[-1.809]]\n",
      "reward: [[-2.478]]\n",
      "reward: [[-2.742]]\n",
      "reward: [[-2.794]]\n",
      "reward: [[-2.82]]\n",
      "reward: [[-2.949]]\n",
      "reward: [[-3.3]]\n",
      "reward: [[-3.841]]\n",
      "reward: [[-4.082]]\n",
      "reward: [[-3.545]]\n",
      "reward: [[-2.816]]\n",
      "reward: [[-3.742]]\n",
      "reward: [[-3.796]]\n",
      "reward: [[-4.41]]\n",
      "reward: [[-4.505]]\n",
      "reward: [[-4.568]]\n",
      "reward: [[-4.571]]\n",
      "reward: [[-4.65]]\n",
      "reward: [[-3.167]]\n",
      "reward: [[-4.717]]\n",
      "reward: [[-3.965]]\n",
      "reward: [[-4.723]]\n",
      "reward: [[-4.789]]\n",
      "reward: [[-4.867]]\n",
      "reward: [[-4.95]]\n",
      "reward: [[-4.513]]\n",
      "reward: [[-2.994]]\n",
      "reward: [[-4.59]]\n",
      "reward: [[-3.034]]\n",
      "reward: [[-4.375]]\n",
      "reward: [[-4.893]]\n",
      "reward: [[-4.742]]\n",
      "reward: [[-4.535]]\n",
      "reward: [[-5.135]]\n",
      "reward: [[-5.275]]\n",
      "reward: [[-5.32]]\n",
      "reward: [[-5.157]]\n",
      "reward: [[-4.506]]\n",
      "reward: [[-5.268]]\n",
      "reward: [[-3.776]]\n",
      "reward: [[-4.583]]\n",
      "reward: [[-3.295]]\n",
      "reward: [[-4.399]]\n",
      "reward: [[-5.072]]\n",
      "reward: [[-4.844]]\n",
      "reward: [[-4.337]]\n",
      "reward: [[-4.134]]\n",
      "reward: [[-3.679]]\n",
      "reward: [[-4.515]]\n",
      "reward: [[-1.959]]\n",
      "reward: [[-2.577]]\n",
      "reward: [[-3.753]]\n",
      "reward: [[-4.1]]\n",
      "reward: [[-2.873]]\n",
      "reward: [[-3.91]]\n",
      "reward: [[-3.704]]\n",
      "reward: [[-3.8]]\n",
      "reward: [[-3.88]]\n",
      "reward: [[-3.88]]\n",
      "reward: [[-3.88]]\n",
      "reward: [[-3.88]]\n",
      "reward: [[-3.88]]\n",
      "reward: [[-3.88]]\n",
      "reward: [[-3.88]]\n",
      "reward: [[-3.88]]\n",
      "reward: [[-3.88]]\n",
      "reward: [[-3.88]]\n",
      "reward: [[-3.88]]\n",
      "reward: [[-3.88]]\n",
      "reward: [[-3.88]]\n",
      "reward: [[-3.88]]\n",
      "reward: [[-3.88]]\n",
      "reward: [[-3.88]]\n",
      "reward: [[-3.88]]\n",
      "reward: [[-3.88]]\n",
      "reward: [[-3.88]]\n",
      "reward: [[-3.907]]\n",
      "reward: [[-3.853]]\n",
      "reward: [[-3.88]]\n",
      "reward: [[-3.88]]\n",
      "reward: [[-3.88]]\n",
      "reward: [[-3.88]]\n",
      "reward: [[-3.88]]\n",
      "reward: [[-3.88]]\n",
      "reward: [[-3.88]]\n",
      "reward: [[-3.88]]\n",
      "reward: [[-3.88]]\n",
      "reward: [[-3.88]]\n",
      "reward: [[-3.88]]\n",
      "reward: [[-3.88]]\n",
      "reward: [[-3.88]]\n",
      "reward: [[-3.88]]\n",
      "reward: [[-3.88]]\n",
      "reward: [[-3.88]]\n",
      "reward: [[-3.88]]\n",
      "reward: [[-3.88]]\n",
      "reward: [[-3.88]]\n",
      "reward: [[-3.88]]\n",
      "reward: [[-3.88]]\n",
      "reward: [[-3.88]]\n",
      "reward: [[-3.88]]\n",
      "reward: [[-3.88]]\n",
      "reward: [[-0.194]]\n",
      "reward: [[0.]]\n",
      "reward: [[-2.552]]\n",
      "reward: [[-3.38]]\n",
      "reward: [[-3.595]]\n",
      "reward: [[-3.7]]\n",
      "reward: [[-3.797]]\n",
      "reward: [[-3.925]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-3.96]]\n",
      "reward: [[-0.198]]\n",
      "reward: [[-0.082]]\n",
      "reward: [[-2.223]]\n",
      "reward: [[-2.929]]\n",
      "reward: [[-3.016]]\n",
      "reward: [[-2.815]]\n",
      "reward: [[-3.214]]\n",
      "reward: [[-3.816]]\n",
      "reward: [[-3.894]]\n",
      "reward: [[-3.94]]\n",
      "reward: [[-3.654]]\n",
      "reward: [[-3.927]]\n",
      "reward: [[-3.94]]\n",
      "reward: [[-3.94]]\n",
      "reward: [[-3.94]]\n",
      "reward: [[-0.197]]\n",
      "reward: [[-0.196]]\n",
      "reward: [[-2.227]]\n",
      "reward: [[-3.336]]\n",
      "reward: [[-3.749]]\n",
      "reward: [[-3.925]]\n",
      "reward: [[-4.031]]\n",
      "reward: [[-4.131]]\n",
      "reward: [[-4.25]]\n",
      "reward: [[-4.26]]\n",
      "reward: [[-4.26]]\n",
      "reward: [[-4.26]]\n",
      "reward: [[-4.26]]\n",
      "reward: [[-4.26]]\n",
      "reward: [[-4.26]]\n",
      "reward: [[-4.26]]\n",
      "reward: [[-4.26]]\n",
      "reward: [[-4.26]]\n",
      "reward: [[-4.26]]\n",
      "500\n",
      "fail to complete the episode\n",
      "5000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import traceback\n",
    "\n",
    "try:\n",
    "    run(episodes=2)\n",
    "except Exception as e:\n",
    "    traceback.print_exc()\n",
    "    traci.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo():\n",
    "    traci.start(gui)\n",
    "    total_time = 0\n",
    "    env = environment()\n",
    "    agent = agents(env.state_size(), env.action_size())\n",
    "    agent.load_policy('model.pt')\n",
    "    step = 0\n",
    "    prevPhase = 0\n",
    "    action = [0]\n",
    "    while traci.simulation.getMinExpectedNumber() > 0 and step < 5e3:\n",
    "        traci.simulationStep()\n",
    "        env.do_action(action)\n",
    "        env.cumulateWaitingTime()\n",
    "        step += 1\n",
    "        if step % 100 == 0:\n",
    "            state = env.getState()\n",
    "            phase = env.getPhase()\n",
    "            \n",
    "            print('reward')\n",
    "            print(env.reward())\n",
    "            print('-------')\n",
    "            print('state:')\n",
    "            print(state)\n",
    "            \n",
    "            action = agent.select_actions(1 , state, True)\n",
    "            print('action: ', action)\n",
    "            env.do_action(action)\n",
    "            \n",
    "\n",
    "    if (step >= 5e3-1):\n",
    "        print('fail to complete the episode')\n",
    "    print('total waiting time of rl traffic light', step)\n",
    "    traci.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': ['GGGGGgrrrrrrrrgGGGGgrrrrrrrGr', 'GGGGGgrrrrrrrrgGGGGgrrrrrrrrr', 'yyyyygrrrrrrrryyyyygrrrrrrrrr', 'rrrrrGrrrrrrrrrrrrrGrrrrrrrrr', 'rrrrryrrrrrrrrrrrrryrrrrrrrrr', 'rrrrrrGGGGGGGgrrrrrrgGGGGggrG', 'rrrrrrGGGGGGGgrrrrrrgGGGGggrr', 'rrrrrryyyyyyyyrrrrrryyyyyyyrr']}\n",
      "{'0': {'31_2', '41_2', '11_1', '21_1', '41_1', '21_3', ':0_w2_0', '11_3', '21_2', '11_2', '11_0', '31_0', '31_3', '41_0', '21_4', '21_0', '41_3', '31_1', ':0_w1_0'}}\n",
      "device available:  cpu\n",
      "action:  [0]\n",
      "reward\n",
      "[[-0.651]]\n",
      "state:\n",
      "[[0.02185697 0.         0.06026622 0.         0.0267037  0.\n",
      "  0.         0.1734619  0.         0.05240541 0.1939     0.14513026\n",
      "  0.07256513 0.08011109 0.04985702 0.13052039 0.08011109 0.08742787\n",
      "  0.        ]]\n",
      "tensor([[-3.7235, -0.7978, -2.6722, -2.3350, -1.4341, -1.1745, -0.6990, -2.5319]])\n",
      "action:  [[6]]\n",
      "reward\n",
      "[[-2.151]]\n",
      "state:\n",
      "[[0.00699423 0.07316813 0.         0.07426161 0.14206366 0.\n",
      "  0.         0.37935105 0.0281294  0.         0.13101353 0.06557091\n",
      "  0.08359043 0.14206366 0.17777778 0.3538678  0.24887845 0.\n",
      "  0.        ]]\n",
      "tensor([[-4.2110, -0.3985, -2.9517, -2.2073, -1.2650, -1.0312, -0.2150, -2.6785]])\n",
      "action:  [[6]]\n",
      "reward\n",
      "[[-5.665]]\n",
      "state:\n",
      "[[0.07431369 0.15327922 0.         0.12939522 0.17998292 0.11251758\n",
      "  0.         0.40887803 0.13052039 0.0262027  0.13101353 0.06557091\n",
      "  0.1045727  0.29267251 0.26216596 0.5113924  0.3669088  0.02185697\n",
      "  0.        ]]\n",
      "tensor([[-5.2686,  0.9649, -3.6568, -2.4634,  0.1822, -1.6724,  0.3410, -3.0887]])\n",
      "action:  [[1]]\n",
      "reward\n",
      "[[-4.954]]\n",
      "state:\n",
      "[[0.03584543 0.00854518 0.07860811 0.05625879 0.0267037  0.\n",
      "  0.         0.649303   0.         0.06812703 0.41714704 0.17485574\n",
      "  0.36423346 0.13351847 0.25983855 0.29817158 0.5640464  0.06557091\n",
      "  0.        ]]\n",
      "tensor([[-1.0836, -0.2474, -3.1685, -1.3489, -1.8504, -2.6894, -2.0946, -2.0560]])\n",
      "action:  [[1]]\n",
      "reward\n",
      "[[-7.467]]\n",
      "state:\n",
      "[[0.11540479 0.14473403 0.2442092  0.05625879 0.         0.00900141\n",
      "  0.         0.6755057  0.         0.2499738  0.59060895 0.2911348\n",
      "  0.5023695  0.09132664 0.3581551  0.07426161 0.63554794 0.10928484\n",
      "  0.        ]]\n",
      "tensor([[ 2.0990,  0.6937, -4.3698, -1.2921, -1.1721, -6.4760, -4.0027, -0.6339]])\n",
      "action:  [[0]]\n",
      "reward\n",
      "[[-13.854]]\n",
      "state:\n",
      "[[0.17310719 0.2867977  0.3914684  0.         0.09720145 0.\n",
      "  0.         0.6755057  0.         0.44020543 0.67760193 0.42227662\n",
      "  0.6282657  0.08865627 0.24563721 0.14064698 0.63554794 0.21070117\n",
      "  0.        ]]\n",
      "tensor([[ 1.6736,  0.2731, -4.4409, -1.4416, -1.4122, -6.2963, -3.7845, -0.3261]])\n",
      "action:  [[0]]\n",
      "reward\n",
      "[[-18.883]]\n",
      "state:\n",
      "[[0.27452353 0.44274727 0.3914684  0.         0.04379406 0.\n",
      "  0.         0.6755057  0.         0.44020543 0.67760193 0.5752754\n",
      "  0.7003936  0.10681479 0.15224816 0.12151898 0.6639182  0.2911348\n",
      "  0.        ]]\n",
      "tensor([[ 0.9169, -0.1533, -4.3188, -1.5431, -1.5482, -5.6083, -3.3401, -0.3581]])\n",
      "action:  [[0]]\n",
      "reward\n",
      "[[-18.757]]\n",
      "state:\n",
      "[[0.5088302  0.5228584  0.3914684  0.         0.16022217 0.\n",
      "  0.         0.6755057  0.         0.44020543 0.67760193 0.62598354\n",
      "  0.7003936  0.03791925 0.14964838 0.11251758 0.6285601  0.46599054\n",
      "  0.        ]]\n",
      "tensor([[-0.2299, -0.9299, -4.2560, -1.7983, -1.5236, -4.7435, -2.6109, -0.2337]])\n",
      "action:  [[0]]\n",
      "reward\n",
      "[[-25.011]]\n",
      "state:\n",
      "[[0.61461794 0.44274727 0.3914684  0.0281294  0.34714803 0.00900141\n",
      "  0.         0.6755057  0.         0.44020543 0.67760193 0.661829\n",
      "  0.7003936  0.08011109 0.         0.12151898 0.6526383  0.5228187\n",
      "  0.        ]]\n",
      "tensor([[-1.1024, -1.0264, -4.0105, -1.6119, -1.0062, -3.6599, -2.2153, -0.9751]])\n",
      "action:  [[7]]\n",
      "reward\n",
      "[[-22.767]]\n",
      "state:\n",
      "[[0.6006295  0.46250802 0.3914684  0.11251758 0.5607776  0.05625879\n",
      "  0.         0.6755057  0.08438819 0.44020543 0.67760193 0.66882324\n",
      "  0.674506   0.21523179 0.0371308  0.3077356  0.6526383  0.52981293\n",
      "  0.        ]]\n",
      "tensor([[-1.9316, -0.9931, -3.6703, -1.6308, -1.0685, -2.8295, -2.0941, -1.7522]])\n",
      "action:  [[1]]\n",
      "reward\n",
      "[[-19.88]]\n",
      "state:\n",
      "[[0.6006295  0.39788505 0.3914684  0.         0.50737023 0.\n",
      "  0.         0.6755057  0.         0.44020543 0.67760193 0.66882324\n",
      "  0.674506   0.34020507 0.01800281 0.14064698 0.6441573  0.52981293\n",
      "  0.        ]]\n",
      "tensor([[-1.7841, -0.7649, -3.1007, -1.5971, -2.2361, -2.2806, -1.8634, -1.9188]])\n",
      "action:  [[1]]\n",
      "reward\n",
      "[[-27.677]]\n",
      "state:\n",
      "[[0.6006295  0.39788505 0.3914684  0.0281294  0.48066652 0.0281294\n",
      "  0.         0.6755057  0.         0.44020543 0.67760193 0.66882324\n",
      "  0.674506   0.42031616 0.0371308  0.05625879 0.6174543  0.52981293\n",
      "  0.        ]]\n",
      "tensor([[-1.3815, -0.7150, -3.0168, -1.7449, -2.9343, -2.6021, -1.9140, -1.5622]])\n",
      "action:  [[1]]\n",
      "reward\n",
      "[[-24.213]]\n",
      "state:\n",
      "[[0.6006295  0.39788505 0.3914684  0.0281294  0.45396283 0.\n",
      "  0.         0.6755057  0.         0.44020543 0.67760193 0.66882324\n",
      "  0.674506   0.2867977  0.0281294  0.1209564  0.6174536  0.52981293\n",
      "  0.        ]]\n",
      "tensor([[-1.5014, -0.7939, -3.2444, -1.6537, -2.2597, -2.6667, -1.9529, -1.6068]])\n",
      "action:  [[1]]\n",
      "reward\n",
      "[[-23.871]]\n",
      "state:\n",
      "[[0.6006295  0.34447768 0.3914684  0.         0.3583636  0.\n",
      "  0.         0.6755057  0.         0.44020543 0.67760193 0.66882324\n",
      "  0.674506   0.30228585 0.         0.         0.49826244 0.52981293\n",
      "  0.        ]]\n",
      "tensor([[-0.9359, -0.8072, -3.3622, -1.9092, -2.8600, -3.4281, -2.1540, -0.9822]])\n",
      "action:  [[1]]\n",
      "reward\n",
      "[[-19.065]]\n",
      "state:\n",
      "[[0.6006295  0.2333903  0.3914684  0.         0.14206366 0.\n",
      "  0.         0.6755057  0.         0.44020543 0.67760193 0.66882324\n",
      "  0.674506   0.16022217 0.         0.         0.42458877 0.52981293\n",
      "  0.        ]]\n",
      "tensor([[-0.2741, -0.7600, -3.5836, -2.1514, -3.3415, -4.4044, -2.5259, -0.3475]])\n",
      "action:  [[0]]\n",
      "reward\n",
      "[[-21.78]]\n",
      "state:\n",
      "[[ 6.0062951e-01 -1.8974118e-16  3.9146841e-01  0.0000000e+00\n",
      "   8.6580608e-03  0.0000000e+00  0.0000000e+00  6.7550570e-01\n",
      "   0.0000000e+00  4.4020543e-01  6.7760193e-01  6.6882324e-01\n",
      "   6.7450601e-01  1.2390515e-01  0.0000000e+00  0.0000000e+00\n",
      "   3.9948729e-01  5.2981293e-01  0.0000000e+00]]\n",
      "tensor([[ 0.5935, -0.4977, -3.7559, -2.3205, -4.0192, -5.4896, -3.1054,  0.1830]])\n",
      "action:  [[0]]\n",
      "reward\n",
      "[[-22.2]]\n",
      "state:\n",
      "[[0.6006295  0.03791925 0.3914684  0.         0.00854518 0.\n",
      "  0.         0.6755057  0.         0.44020543 0.67760193 0.66882324\n",
      "  0.674506   0.05340739 0.         0.         0.05340739 0.52981293\n",
      "  0.        ]]\n",
      "tensor([[ 0.5230, -0.4709, -3.6822, -2.2483, -3.9956, -5.2844, -3.0321,  0.0576]])\n",
      "action:  [[0]]\n",
      "reward\n",
      "[[-22.2]]\n",
      "state:\n",
      "[[0.6006295  0.06462295 0.3914684  0.         0.         0.\n",
      "  0.         0.6755057  0.         0.44020543 0.67760193 0.66882324\n",
      "  0.674506   0.05340739 0.         0.         0.0267037  0.52981293\n",
      "  0.        ]]\n",
      "tensor([[ 0.4644, -0.5051, -3.6545, -2.2808, -4.0439, -5.2333, -2.9859,  0.0589]])\n",
      "action:  [[0]]\n",
      "reward\n",
      "[[-22.2]]\n",
      "state:\n",
      "[[ 6.0062951e-01 -1.8974118e-16  3.9146841e-01  0.0000000e+00\n",
      "   1.7090365e-02  0.0000000e+00  0.0000000e+00  6.7550570e-01\n",
      "   0.0000000e+00  4.4020543e-01  6.7760193e-01  6.6882324e-01\n",
      "   6.7450601e-01  9.9871822e-02  0.0000000e+00  0.0000000e+00\n",
      "   7.0497759e-02  5.2981293e-01  0.0000000e+00]]\n",
      "tensor([[ 0.7370, -0.3749, -3.6988, -2.2555, -4.1655, -5.4976, -3.1733,  0.1366]])\n",
      "action:  [[0]]\n",
      "reward\n",
      "[[-22.2]]\n",
      "state:\n",
      "[[ 6.0062951e-01 -1.8974118e-16  3.9146841e-01  0.0000000e+00\n",
      "   1.7090365e-02  0.0000000e+00  0.0000000e+00  6.7550570e-01\n",
      "   0.0000000e+00  4.4020543e-01  6.7760193e-01  6.6882324e-01\n",
      "   6.7450601e-01  1.1535997e-01  0.0000000e+00  0.0000000e+00\n",
      "   8.5451826e-03  5.2981293e-01  0.0000000e+00]]\n",
      "tensor([[ 0.9050, -0.3014, -3.7276, -2.2489, -4.2498, -5.6689, -3.2865,  0.1963]])\n",
      "action:  [[0]]\n",
      "reward\n",
      "[[-22.2]]\n",
      "state:\n",
      "[[0.6006295  0.0267037  0.3914684  0.         0.         0.\n",
      "  0.         0.6755057  0.         0.44020543 0.67760193 0.66882324\n",
      "  0.674506   0.08865627 0.         0.         0.0267037  0.52981293\n",
      "  0.        ]]\n",
      "tensor([[ 0.6885, -0.4057, -3.6792, -2.2831, -4.1995, -5.4586, -3.1347,  0.1410]])\n",
      "action:  [[0]]\n",
      "reward\n",
      "[[-22.2]]\n",
      "state:\n",
      "[[ 6.0062951e-01 -1.9448469e-16  3.9146841e-01  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00  0.0000000e+00  6.7550570e-01\n",
      "   0.0000000e+00  4.4020543e-01  6.7760193e-01  6.6882324e-01\n",
      "   6.7450601e-01  8.0111086e-02  0.0000000e+00  0.0000000e+00\n",
      "   8.0111086e-02  5.2981293e-01  0.0000000e+00]]\n",
      "tensor([[ 0.7098, -0.3901, -3.6973, -2.2586, -4.1481, -5.4741, -3.1543,  0.1318]])\n",
      "action:  [[0]]\n",
      "reward\n",
      "[[-22.2]]\n",
      "state:\n",
      "[[ 6.0062951e-01 -1.9448469e-16  3.9146841e-01  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00  0.0000000e+00  6.7550570e-01\n",
      "   0.0000000e+00  4.4020543e-01  6.7760193e-01  6.6882324e-01\n",
      "   6.7450601e-01  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   0.0000000e+00  5.2981293e-01  0.0000000e+00]]\n",
      "tensor([[ 0.6346, -0.4036, -3.7694, -2.1497, -3.7829, -5.3829, -3.1278,  0.0387]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action:  [[0]]\n",
      "reward\n",
      "[[-22.2]]\n",
      "state:\n",
      "[[ 6.0062951e-01 -1.9448469e-16  3.9146841e-01  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00  0.0000000e+00  6.7550570e-01\n",
      "   0.0000000e+00  4.4020543e-01  6.7760193e-01  6.6882324e-01\n",
      "   6.7450601e-01  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   0.0000000e+00  5.2981293e-01  0.0000000e+00]]\n",
      "tensor([[ 0.6346, -0.4036, -3.7694, -2.1497, -3.7829, -5.3829, -3.1278,  0.0387]])\n",
      "action:  [[0]]\n",
      "reward\n",
      "[[-22.2]]\n",
      "state:\n",
      "[[ 6.0062951e-01 -1.9448469e-16  3.9146841e-01  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00  0.0000000e+00  6.7550570e-01\n",
      "   0.0000000e+00  4.4020543e-01  6.7760193e-01  6.6882324e-01\n",
      "   6.7450601e-01  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   0.0000000e+00  5.2981293e-01  0.0000000e+00]]\n",
      "tensor([[ 0.6346, -0.4036, -3.7694, -2.1497, -3.7829, -5.3829, -3.1278,  0.0387]])\n",
      "action:  [[0]]\n",
      "reward\n",
      "[[-22.2]]\n",
      "state:\n",
      "[[ 6.0062951e-01 -1.9448469e-16  3.9146841e-01  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00  0.0000000e+00  6.7550570e-01\n",
      "   0.0000000e+00  4.4020543e-01  6.7760193e-01  6.6882324e-01\n",
      "   6.7450601e-01  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   0.0000000e+00  5.2981293e-01  0.0000000e+00]]\n",
      "tensor([[ 0.6346, -0.4036, -3.7694, -2.1497, -3.7829, -5.3829, -3.1278,  0.0387]])\n",
      "action:  [[0]]\n",
      "reward\n",
      "[[-22.2]]\n",
      "state:\n",
      "[[ 6.0062951e-01 -1.9448469e-16  3.9146841e-01  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00  0.0000000e+00  6.7550570e-01\n",
      "   0.0000000e+00  4.4020543e-01  6.7760193e-01  6.6882324e-01\n",
      "   6.7450601e-01  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   0.0000000e+00  5.2981293e-01  0.0000000e+00]]\n",
      "tensor([[ 0.6346, -0.4036, -3.7694, -2.1497, -3.7829, -5.3829, -3.1278,  0.0387]])\n",
      "action:  [[0]]\n",
      "reward\n",
      "[[-22.2]]\n",
      "state:\n",
      "[[ 6.0062951e-01 -1.9448469e-16  3.9146841e-01  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00  0.0000000e+00  6.7550570e-01\n",
      "   0.0000000e+00  4.4020543e-01  6.7760193e-01  6.6882324e-01\n",
      "   6.7450601e-01  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   0.0000000e+00  5.2981293e-01  0.0000000e+00]]\n",
      "tensor([[ 0.6346, -0.4036, -3.7694, -2.1497, -3.7829, -5.3829, -3.1278,  0.0387]])\n",
      "action:  [[0]]\n",
      "reward\n",
      "[[-22.2]]\n",
      "state:\n",
      "[[ 6.0062951e-01 -1.9448469e-16  3.9146841e-01  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00  0.0000000e+00  6.7550570e-01\n",
      "   0.0000000e+00  4.4020543e-01  6.7760193e-01  6.6882324e-01\n",
      "   6.7450601e-01  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   0.0000000e+00  5.2981293e-01  0.0000000e+00]]\n",
      "tensor([[ 0.6346, -0.4036, -3.7694, -2.1497, -3.7829, -5.3829, -3.1278,  0.0387]])\n",
      "action:  [[0]]\n",
      "reward\n",
      "[[-22.2]]\n",
      "state:\n",
      "[[ 6.0062951e-01 -1.9448469e-16  3.9146841e-01  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00  0.0000000e+00  6.7550570e-01\n",
      "   0.0000000e+00  4.4020543e-01  6.7760193e-01  6.6882324e-01\n",
      "   6.7450601e-01  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   0.0000000e+00  5.2981293e-01  0.0000000e+00]]\n",
      "tensor([[ 0.6346, -0.4036, -3.7694, -2.1497, -3.7829, -5.3829, -3.1278,  0.0387]])\n",
      "action:  [[0]]\n",
      "reward\n",
      "[[-22.2]]\n",
      "state:\n",
      "[[ 6.0062951e-01 -1.9448469e-16  3.9146841e-01  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00  0.0000000e+00  6.7550570e-01\n",
      "   0.0000000e+00  4.4020543e-01  6.7760193e-01  6.6882324e-01\n",
      "   6.7450601e-01  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   0.0000000e+00  5.2981293e-01  0.0000000e+00]]\n",
      "tensor([[ 0.6346, -0.4036, -3.7694, -2.1497, -3.7829, -5.3829, -3.1278,  0.0387]])\n",
      "action:  [[0]]\n",
      "reward\n",
      "[[-22.2]]\n",
      "state:\n",
      "[[ 6.0062951e-01 -1.9448469e-16  3.9146841e-01  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00  0.0000000e+00  6.7550570e-01\n",
      "   0.0000000e+00  4.4020543e-01  6.7760193e-01  6.6882324e-01\n",
      "   6.7450601e-01  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   0.0000000e+00  5.2981293e-01  0.0000000e+00]]\n",
      "tensor([[ 0.6346, -0.4036, -3.7694, -2.1497, -3.7829, -5.3829, -3.1278,  0.0387]])\n",
      "action:  [[0]]\n",
      "reward\n",
      "[[-22.2]]\n",
      "state:\n",
      "[[ 6.0062951e-01 -1.9448469e-16  3.9146841e-01  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00  0.0000000e+00  6.7550570e-01\n",
      "   0.0000000e+00  4.4020543e-01  6.7760193e-01  6.6882324e-01\n",
      "   6.7450601e-01  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   0.0000000e+00  5.2981293e-01  0.0000000e+00]]\n",
      "tensor([[ 0.6346, -0.4036, -3.7694, -2.1497, -3.7829, -5.3829, -3.1278,  0.0387]])\n",
      "action:  [[0]]\n",
      "reward\n",
      "[[-22.2]]\n",
      "state:\n",
      "[[ 6.0062951e-01 -1.9448469e-16  3.9146841e-01  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00  0.0000000e+00  6.7550570e-01\n",
      "   0.0000000e+00  4.4020543e-01  6.7760193e-01  6.6882324e-01\n",
      "   6.7450601e-01  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   0.0000000e+00  5.2981293e-01  0.0000000e+00]]\n",
      "tensor([[ 0.6346, -0.4036, -3.7694, -2.1497, -3.7829, -5.3829, -3.1278,  0.0387]])\n",
      "action:  [[0]]\n",
      "reward\n",
      "[[-22.2]]\n",
      "state:\n",
      "[[ 6.0062951e-01 -1.9448469e-16  3.9146841e-01  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00  0.0000000e+00  6.7550570e-01\n",
      "   0.0000000e+00  4.4020543e-01  6.7760193e-01  6.6882324e-01\n",
      "   6.7450601e-01  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   0.0000000e+00  5.2981293e-01  0.0000000e+00]]\n",
      "tensor([[ 0.6346, -0.4036, -3.7694, -2.1497, -3.7829, -5.3829, -3.1278,  0.0387]])\n",
      "action:  [[0]]\n",
      "reward\n",
      "[[-22.2]]\n",
      "state:\n",
      "[[ 6.0062951e-01 -1.9448469e-16  3.9146841e-01  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00  0.0000000e+00  6.7550570e-01\n",
      "   0.0000000e+00  4.4020543e-01  6.7760193e-01  6.6882324e-01\n",
      "   6.7450601e-01  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   0.0000000e+00  5.2981293e-01  0.0000000e+00]]\n",
      "tensor([[ 0.6346, -0.4036, -3.7694, -2.1497, -3.7829, -5.3829, -3.1278,  0.0387]])\n",
      "action:  [[0]]\n",
      "reward\n",
      "[[-22.2]]\n",
      "state:\n",
      "[[ 6.0062951e-01 -1.9448469e-16  3.9146841e-01  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00  0.0000000e+00  6.7550570e-01\n",
      "   0.0000000e+00  4.4020543e-01  6.7760193e-01  6.6882324e-01\n",
      "   6.7450601e-01  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   0.0000000e+00  5.2981293e-01  0.0000000e+00]]\n",
      "tensor([[ 0.6346, -0.4036, -3.7694, -2.1497, -3.7829, -5.3829, -3.1278,  0.0387]])\n",
      "action:  [[0]]\n",
      "reward\n",
      "[[-22.2]]\n",
      "state:\n",
      "[[ 6.0062951e-01 -1.9448469e-16  3.9146841e-01  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00  0.0000000e+00  6.7550570e-01\n",
      "   0.0000000e+00  4.4020543e-01  6.7760193e-01  6.6882324e-01\n",
      "   6.7450601e-01  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   0.0000000e+00  5.2981293e-01  0.0000000e+00]]\n",
      "tensor([[ 0.6346, -0.4036, -3.7694, -2.1497, -3.7829, -5.3829, -3.1278,  0.0387]])\n",
      "action:  [[0]]\n",
      "reward\n",
      "[[-22.2]]\n",
      "state:\n",
      "[[ 6.0062951e-01 -1.9448469e-16  3.9146841e-01  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00  0.0000000e+00  6.7550570e-01\n",
      "   0.0000000e+00  4.4020543e-01  6.7760193e-01  6.6882324e-01\n",
      "   6.7450601e-01  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   0.0000000e+00  5.2981293e-01  0.0000000e+00]]\n",
      "tensor([[ 0.6346, -0.4036, -3.7694, -2.1497, -3.7829, -5.3829, -3.1278,  0.0387]])\n",
      "action:  [[0]]\n",
      "reward\n",
      "[[-22.2]]\n",
      "state:\n",
      "[[ 6.0062951e-01 -1.9448469e-16  3.9146841e-01  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00  0.0000000e+00  6.7550570e-01\n",
      "   0.0000000e+00  4.4020543e-01  6.7760193e-01  6.6882324e-01\n",
      "   6.7450601e-01  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   0.0000000e+00  5.2981293e-01  0.0000000e+00]]\n",
      "tensor([[ 0.6346, -0.4036, -3.7694, -2.1497, -3.7829, -5.3829, -3.1278,  0.0387]])\n",
      "action:  [[0]]\n",
      "reward\n",
      "[[-22.2]]\n",
      "state:\n",
      "[[ 6.0062951e-01 -1.9448469e-16  3.9146841e-01  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00  0.0000000e+00  6.7550570e-01\n",
      "   0.0000000e+00  4.4020543e-01  6.7760193e-01  6.6882324e-01\n",
      "   6.7450601e-01  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   0.0000000e+00  5.2981293e-01  0.0000000e+00]]\n",
      "tensor([[ 0.6346, -0.4036, -3.7694, -2.1497, -3.7829, -5.3829, -3.1278,  0.0387]])\n",
      "action:  [[0]]\n",
      "reward\n",
      "[[-22.2]]\n",
      "state:\n",
      "[[ 6.0062951e-01 -1.9448469e-16  3.9146841e-01  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00  0.0000000e+00  6.7550570e-01\n",
      "   0.0000000e+00  4.4020543e-01  6.7760193e-01  6.6882324e-01\n",
      "   6.7450601e-01  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   0.0000000e+00  5.2981293e-01  0.0000000e+00]]\n",
      "tensor([[ 0.6346, -0.4036, -3.7694, -2.1497, -3.7829, -5.3829, -3.1278,  0.0387]])\n",
      "action:  [[0]]\n",
      "reward\n",
      "[[-22.2]]\n",
      "state:\n",
      "[[ 6.0062951e-01 -1.9448469e-16  3.9146841e-01  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00  0.0000000e+00  6.7550570e-01\n",
      "   0.0000000e+00  4.4020543e-01  6.7760193e-01  6.6882324e-01\n",
      "   6.7450601e-01  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   0.0000000e+00  5.2981293e-01  0.0000000e+00]]\n",
      "tensor([[ 0.6346, -0.4036, -3.7694, -2.1497, -3.7829, -5.3829, -3.1278,  0.0387]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action:  [[0]]\n",
      "reward\n",
      "[[-22.2]]\n",
      "state:\n",
      "[[ 6.0062951e-01 -1.9448469e-16  3.9146841e-01  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00  0.0000000e+00  6.7550570e-01\n",
      "   0.0000000e+00  4.4020543e-01  6.7760193e-01  6.6882324e-01\n",
      "   6.7450601e-01  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   0.0000000e+00  5.2981293e-01  0.0000000e+00]]\n",
      "tensor([[ 0.6346, -0.4036, -3.7694, -2.1497, -3.7829, -5.3829, -3.1278,  0.0387]])\n",
      "action:  [[0]]\n",
      "reward\n",
      "[[-22.2]]\n",
      "state:\n",
      "[[ 6.0062951e-01 -1.9448469e-16  3.9146841e-01  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00  0.0000000e+00  6.7550570e-01\n",
      "   0.0000000e+00  4.4020543e-01  6.7760193e-01  6.6882324e-01\n",
      "   6.7450601e-01  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   0.0000000e+00  5.2981293e-01  0.0000000e+00]]\n",
      "tensor([[ 0.6346, -0.4036, -3.7694, -2.1497, -3.7829, -5.3829, -3.1278,  0.0387]])\n",
      "action:  [[0]]\n",
      "reward\n",
      "[[-22.2]]\n",
      "state:\n",
      "[[ 6.0062951e-01 -1.9448469e-16  3.9146841e-01  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00  0.0000000e+00  6.7550570e-01\n",
      "   0.0000000e+00  4.4020543e-01  6.7760193e-01  6.6882324e-01\n",
      "   6.7450601e-01  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   0.0000000e+00  5.2981293e-01  0.0000000e+00]]\n",
      "tensor([[ 0.6346, -0.4036, -3.7694, -2.1497, -3.7829, -5.3829, -3.1278,  0.0387]])\n",
      "action:  [[0]]\n",
      "reward\n",
      "[[-22.2]]\n",
      "state:\n",
      "[[ 6.0062951e-01 -1.9448469e-16  3.9146841e-01  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00  0.0000000e+00  6.7550570e-01\n",
      "   0.0000000e+00  4.4020543e-01  6.7760193e-01  6.6882324e-01\n",
      "   6.7450601e-01  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   0.0000000e+00  5.2981293e-01  0.0000000e+00]]\n",
      "tensor([[ 0.6346, -0.4036, -3.7694, -2.1497, -3.7829, -5.3829, -3.1278,  0.0387]])\n",
      "action:  [[0]]\n",
      "reward\n",
      "[[-22.2]]\n",
      "state:\n",
      "[[ 6.0062951e-01 -1.9448469e-16  3.9146841e-01  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00  0.0000000e+00  6.7550570e-01\n",
      "   0.0000000e+00  4.4020543e-01  6.7760193e-01  6.6882324e-01\n",
      "   6.7450601e-01  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   0.0000000e+00  5.2981293e-01  0.0000000e+00]]\n",
      "tensor([[ 0.6346, -0.4036, -3.7694, -2.1497, -3.7829, -5.3829, -3.1278,  0.0387]])\n",
      "action:  [[0]]\n",
      "reward\n",
      "[[-22.2]]\n",
      "state:\n",
      "[[ 6.0062951e-01 -1.9448469e-16  3.9146841e-01  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00  0.0000000e+00  6.7550570e-01\n",
      "   0.0000000e+00  4.4020543e-01  6.7760193e-01  6.6882324e-01\n",
      "   6.7450601e-01  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   0.0000000e+00  5.2981293e-01  0.0000000e+00]]\n",
      "tensor([[ 0.6346, -0.4036, -3.7694, -2.1497, -3.7829, -5.3829, -3.1278,  0.0387]])\n",
      "action:  [[0]]\n",
      "reward\n",
      "[[-22.2]]\n",
      "state:\n",
      "[[ 6.0062951e-01 -1.9448469e-16  3.9146841e-01  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00  0.0000000e+00  6.7550570e-01\n",
      "   0.0000000e+00  4.4020543e-01  6.7760193e-01  6.6882324e-01\n",
      "   6.7450601e-01  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   0.0000000e+00  5.2981293e-01  0.0000000e+00]]\n",
      "tensor([[ 0.6346, -0.4036, -3.7694, -2.1497, -3.7829, -5.3829, -3.1278,  0.0387]])\n",
      "fail to complete the episode\n",
      "total waiting time of rl traffic light 5000\n"
     ]
    }
   ],
   "source": [
    "demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
